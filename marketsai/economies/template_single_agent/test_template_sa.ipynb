{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Test suite for env_template_sa"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from marketsai.economies.template_single_agent.env_template_sa import TemplateSA\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# environment config\n",
    "env_config = {\n",
    "    \"horizon\": 200,\n",
    "    \"eval_mode\": True,\n",
    "    \"analysis_mode\": False,\n",
    "    \"simul_mode\": False,\n",
    "    \"max_action\": 0.6,\n",
    "    # \"rew_mean\": 0.9200565795467147,\n",
    "    # \"rew_std\": 0.3003009455512563,\n",
    "    \"rew_mean\": 0,\n",
    "    \"rew_std\": 1,\n",
    "    \"parameters\": {\n",
    "        \"alpha\": 0.36,\n",
    "        \"delta\": 0.025,\n",
    "        \"beta\": 0.99,\n",
    "    },\n",
    "}\n",
    "\n",
    "def process_rewards(r, BETA):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        running_add = running_add * BETA + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r[0]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# validate spaces\n",
    "env = TemplateSA(env_config=env_config)\n",
    "print(\n",
    "    \"action space type:\",\n",
    "    type(env.action_space.sample()),\n",
    "    \"action space sample:\",\n",
    "    env.action_space.sample(),\n",
    ")\n",
    "print(\n",
    "    \"obs space type:\",\n",
    "    type(env.observation_space.sample()),\n",
    "    \"obs space sample:\",\n",
    "    env.observation_space.sample(),\n",
    ")\n",
    "obs_init = env.reset()\n",
    "print(\n",
    "    \"obs_init contained in obs_space?\",\n",
    "    env.observation_space.contains(obs_init),\n",
    ")\n",
    "if not env.observation_space.contains(obs_init):\n",
    "    print(obs_init)\n",
    "print(\n",
    "    \"random number in [-1,1] contained in action_space?\",\n",
    "    env.action_space.contains(np.array([np.random.uniform(-1, 1)])),\n",
    ")\n",
    "obs, rew, done, info = env.step(env.action_space.sample())\n",
    "print(\n",
    "    \"obs after step contained in obs space?\",\n",
    "    env.observation_space.contains(obs),\n",
    "        )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_timing = {\n",
    "    \"time_init\": [],\n",
    "    \"time_reset\": [],\n",
    "    \"time_step\": [],\n",
    "    \"max_passthrough\": [],\n",
    "}\n",
    "\n",
    "time_preinit = time.time()\n",
    "env = TemplateSA(env_config=env_config)\n",
    "time_postinit = time.time()\n",
    "env.reset()\n",
    "time_postreset = time.time()\n",
    "obs, rew, done, info = env.step(np.array([np.random.uniform(-1, 1)]))\n",
    "time_poststep = time.time()\n",
    "\n",
    "data_timing[\"time_init\"].append((time_postinit - time_preinit) * 1000)\n",
    "data_timing[\"time_reset\"].append((time_postreset - time_postinit) * 1000)\n",
    "data_timing[\"time_step\"].append((time_poststep - time_postreset) * 1000)\n",
    "data_timing[\"max_passthrough\"].append(1 / (time_poststep - time_postreset))\n",
    "print(data_timing)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# simulate\n",
    "SIMUL_PERIODS = 1000000\n",
    "env = TemplateSA(env_config=env_config)\n",
    "print(\"steady_state\", env.k_ss)\n",
    "cap_stats, rew_stats, rew_disc_stats = env.random_sample(SIMUL_PERIODS)\n",
    "print(\"[cap_max, cap_min, cap_mean, cap_std]:\", cap_stats, \n",
    "    \"\\n\" + \"[rew_max, rew_min, rew_mean, rew_std:]\", rew_stats,\n",
    "    \"\\n\" + \"[rew_disc_max, rew_disc_min, rew_disc_mean, rew_disc_std:]\", rew_disc_stats,)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "steady_state 37.989253538152255\n",
      "[cap_max, cap_min, cap_mean, cap_std]: [67.1015911841568, 17.10507043429091, 38.886988509332156, 8.63749487933472] \n",
      "[rew_max, rew_min, rew_mean, rew_std:] [1.5681634546338266, 0.022497834130237468, 0.9138102408757718, 0.2939973479751011] \n",
      "[rew_disc_max, rew_disc_min, rew_disc_mean, rew_disc_std:] [76.31057981929418, 72.65286798907721, 74.78982161488845, 0.4214407435202891]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# run analysis mode\n",
    "env_config_analysis = env_config.copy()\n",
    "env_config_analysis[\"analysis_mode\"] = True\n",
    "env = TemplateSA(env_config=env_config_analysis)\n",
    "k_list = []\n",
    "rew_list = []\n",
    "shock_list = []\n",
    "\n",
    "env.reset()\n",
    "for t in range(1000):\n",
    "    if t % 1000 == 0:\n",
    "        obs = env.reset()\n",
    "    obs, rew, done, info = env.step(env.action_space.sample())\n",
    "    shock_list.append(env.obs_global[1])\n",
    "    k_list.append(info[\"capital\"])\n",
    "    rew_list.append(info[\"rewards\"])\n",
    "disc_rew = process_rewards(rew_list,0.99)\n",
    "print(\"Discounted Rewards\", disc_rew,\n",
    "    \"\\n\"+\"cap_stats:\",\n",
    "    [\n",
    "        np.max(k_list),\n",
    "        np.min(k_list),\n",
    "        np.mean(k_list),\n",
    "        np.std(k_list),\n",
    "    ],\n",
    "    \"\\n\"+\"reward_stats:\",\n",
    "    [np.max(rew_list), np.min(rew_list), np.mean(rew_list), np.std(rew_list)],\n",
    ")\n",
    "plt.plot(shock_list)\n",
    "plt.legend([\"shock\"])\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# run evaluation mode\n",
    "env_config_eval = env_config.copy()\n",
    "env_config_eval[\"eval_mode\"] = True\n",
    "env_config_eval[\"simul_mode\"] = True\n",
    "env = TemplateSA(env_config=env_config_eval)\n",
    "k_list = []\n",
    "rew_list = []\n",
    "shock_list = []\n",
    "\n",
    "env.reset()\n",
    "for t in range(200):\n",
    "    if t % 200 == 0:\n",
    "        obs = env.reset()\n",
    "    obs, rew, done, info = env.step(env.action_space.sample())\n",
    "    # print(obs, \"\\n\", rew, \"\\n\", done, \"\\n\", info)\n",
    "\n",
    "    k_list.append(info[\"capital\"])\n",
    "    shock_list.append(env.obs_global[1])\n",
    "    rew_list.append(info[\"rewards\"])\n",
    "print(\n",
    "    \"cap_stats:\",\n",
    "    [\n",
    "        np.max(k_list),\n",
    "        np.min(k_list),\n",
    "        np.mean(k_list),\n",
    "        np.std(k_list),\n",
    "    ],\n",
    "    \"reward_stats:\",\n",
    "    [np.max(rew_list), np.min(rew_list), np.mean(rew_list), np.std(rew_list)],\n",
    ")\n",
    "\n",
    "plt.plot(shock_list)\n",
    "plt.legend([\"shock\"])\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit ('marketsai-reVLCGV_-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "1c99579f0a861f1ade1e1abc4784a15ca138f424327e789e2dba1116d0806699"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}